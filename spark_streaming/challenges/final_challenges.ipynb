{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/margaridagomes/dataeng-basic-course/blob/main/spark_streaming/challenges/final_challenges.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_GBE9UsyxwK"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9LeYFsPTjAb"
      },
      "source": [
        "# Setting up PySpark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYXeODL0T1fO",
        "outputId": "497c35ac-6d8b-48a2-a30c-63ce58f66829"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "%pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rcybt71kTDNt"
      },
      "source": [
        "# Context\n",
        "Message events are coming from platform message broker (kafka, pubsub, kinesis...).\n",
        "You need to process the data according to the requirements.\n",
        "\n",
        "Message schema:\n",
        "- timestamp\n",
        "- value\n",
        "- event_type\n",
        "- message_id\n",
        "- country_id\n",
        "- user_id\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkyPORKNSYvV"
      },
      "source": [
        "# Challenge 1\n",
        "\n",
        "Step 1\n",
        "- Change exising producer\n",
        "\t- Change parquet location to \"/content/lake/bronze/messages/data\"\n",
        "\t- Add checkpoint (/content/lake/bronze/messages/checkpoint)\n",
        "\t- Delete /content/lake/bronze/messages and reprocess data\n",
        "\t- For reprocessing, run the streaming for at least 1 minute, then stop it\n",
        "\n",
        "Step 2\n",
        "- Implement new stream job to read from messages in bronze layer and split result in two locations\n",
        "\t- \"messages_corrupted\"\n",
        "\t\t- logic: event_status is null, empty or equal to \"NONE\"\n",
        "    - extra logic: add country name by joining message with countries dataset\n",
        "\t\t- partition by \"date\" -extract it from timestamp\n",
        "\t\t- location: /content/lake/silver/messages_corrupted/data\n",
        "\n",
        "\t- \"messages\"\n",
        "\t\t- logic: not corrupted data\n",
        "\t\t- extra logic: add country name by joining message with countries dataset\n",
        "\t\t- partition by \"date\" -extract it from timestamp\n",
        "\t\t- location: /content/lake/silver/messages/data\n",
        "\n",
        "\t- technical requirements\n",
        "\t\t- add checkpint (choose location)\n",
        "\t\t- use StructSchema\n",
        "\t\t- Set trigger interval to 5 seconds\n",
        "\t\t- run streaming for at least 20 seconds, then stop it\n",
        "\n",
        "\t- alternatives\n",
        "\t\t- implementing single streaming job with foreach/- foreachBatch logic to write into two locations\n",
        "\t\t- implementing two streaming jobs, one for messages and another for messages_corrupted\n",
        "\t\t- (paying attention on the paths and checkpoints)\n",
        "\n",
        "\n",
        "  - Check results:\n",
        "    - results from messages in bronze layer should match with the sum of messages+messages_corrupted in the silver layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Udk3tohSaXOH",
        "outputId": "57d7413a-ddc4-4cf9-c748-ba7c58769eb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faker\n",
            "  Downloading faker-37.4.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: tzdata in /usr/local/lib/python3.11/dist-packages (from faker) (2025.2)\n",
            "Downloading faker-37.4.0-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faker\n",
            "Successfully installed faker-37.4.0\n"
          ]
        }
      ],
      "source": [
        "%pip install faker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDGMKwBdi1qy"
      },
      "source": [
        "# Producer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tPCOdivrfhYh",
        "outputId": "d64c5b30-ab8c-4fdb-e5bf-7eeeea967874"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 148,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql import DataFrame\n",
        "from faker import Faker\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName('Test streaming').getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "\n",
        "fake = Faker()\n",
        "messages = [fake.uuid4() for _ in range(50)]\n",
        "\n",
        "def enrich_data(df, messages=messages):\n",
        "  fake = Faker()\n",
        "  new_columns = {\n",
        "      'event_type': F.lit(fake.random_element(elements=('OPEN', 'RECEIVED', 'SENT', 'CREATED', 'CLICKED', '', 'NONE'))),\n",
        "      'message_id': F.lit(fake.random_element(elements=messages)),\n",
        "      'channel': F.lit(fake.random_element(elements=('CHAT', 'EMAIL', 'SMS', 'PUSH', 'OTHER'))),\n",
        "      'country_id': F.lit(fake.random_int(min=2000, max=2015)),\n",
        "      'user_id': F.lit(fake.random_int(min=1000, max=1050)),\n",
        "  }\n",
        "  df = df.withColumns(new_columns)\n",
        "  return df\n",
        "\n",
        "def insert_messages(df: DataFrame, batch_id):\n",
        "  enrich = enrich_data(df)\n",
        "  enrich.write.mode(\"append\").format(\"parquet\").save(\"content/lake/bronze/messages\")\n",
        "\n",
        "# read stream\n",
        "df_stream = spark.readStream.format(\"rate\").option(\"rowsPerSecond\", 1).load()\n",
        "\n",
        "# write stream\n",
        "query = (df_stream.writeStream\n",
        ".outputMode('append')\n",
        ".trigger(processingTime='1 seconds')\n",
        ".foreachBatch(insert_messages)\n",
        ".start()\n",
        ")\n",
        "\n",
        "query.awaitTermination(60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KNyUK3yplDhg"
      },
      "outputs": [],
      "source": [
        "query.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZWQExsnzlMFe"
      },
      "outputs": [],
      "source": [
        "df = spark.read.format(\"parquet\").load(\"content/lake/bronze/messages/*\")\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# === CHALLENGE 1 - STEP 1: PRODUCER ==="
      ],
      "metadata": {
        "id": "etGr5pBsqoCf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql import DataFrame, SparkSession\n",
        "from faker import Faker\n",
        "\n",
        "# ----------------------------------------\n",
        "# Spark session initialization and config\n",
        "# ----------------------------------------\n",
        "def validate_and_initialize_spark():\n",
        "    \"\"\"\n",
        "    Validate and initialize Spark session with proper configuration.\n",
        "    If a Spark session is already active, reuse it.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Check if Spark session already exists\n",
        "        existing_spark = SparkSession.getActiveSession()\n",
        "\n",
        "        if existing_spark:\n",
        "            spark = existing_spark\n",
        "        else:\n",
        "            # Initialize Spark Session with settings for streaming\n",
        "            spark = SparkSession.builder.appName(\"StreamingETLProducer\").getOrCreate()\n",
        "\n",
        "        return spark\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Spark session validation failed: {e}\")\n",
        "        raise e\n",
        "\n",
        "# Initialize and validate Spark session\n",
        "spark = validate_and_initialize_spark()\n",
        "sc = spark.sparkContext\n",
        "\n",
        "# ----------------------------------------\n",
        "# Clean up and prepare data directories\n",
        "# ----------------------------------------\n",
        "def cleanup_and_setup_directories():\n",
        "    \"\"\"\n",
        "    Clean up existing directories and create a fresh structure for the bronze/messages layer.\n",
        "    Removes any previous run's data and ensures a clean slate for new data.\n",
        "    \"\"\"\n",
        "    base_dir = \"content/lake/bronze/messages\"\n",
        "\n",
        "    # Remove existing bronze directory as required\n",
        "    if os.path.exists(base_dir):\n",
        "        shutil.rmtree(base_dir)\n",
        "\n",
        "    # Create new directory structure with updated paths\n",
        "    os.makedirs(os.path.join(base_dir, \"data\"), exist_ok=True)\n",
        "    os.makedirs(os.path.join(base_dir, \"checkpoint\"), exist_ok=True)\n",
        "\n",
        "# ----------------------------------------\n",
        "# Producer: Generate and stream fake data\n",
        "# ----------------------------------------\n",
        "def run_updated_producer():\n",
        "    \"\"\"\n",
        "    Run the streaming producer with new configuration.\n",
        "    Generates fake message events and writes them in micro-batches to Parquet files using Spark Structured Streaming.\n",
        "    Runs for 1 minute.\n",
        "    \"\"\"\n",
        "    # Generate fake messages (as in original code)\n",
        "    fake = Faker()\n",
        "    messages = [fake.uuid4() for _ in range(50)]\n",
        "\n",
        "    def enrich_data(df, messages=messages):\n",
        "        \"\"\"\n",
        "        Enriches a DataFrame with random event metadata using Faker.\n",
        "        Adds event_type, message_id, channel, country_id, and user_id.\n",
        "        \"\"\"\n",
        "        fake = Faker()\n",
        "        new_columns = {\n",
        "            'event_type': F.lit(fake.random_element(elements=('OPEN', 'RECEIVED', 'SENT', 'CREATED', 'CLICKED', '', 'NONE'))),\n",
        "            'message_id': F.lit(fake.random_element(elements=messages)),\n",
        "            'channel': F.lit(fake.random_element(elements=('CHAT', 'EMAIL', 'SMS', 'PUSH', 'OTHER'))),\n",
        "            'country_id': F.lit(fake.random_int(min=2000, max=2015)),\n",
        "            'user_id': F.lit(fake.random_int(min=1000, max=1050)),\n",
        "        }\n",
        "        df = df.withColumns(new_columns)\n",
        "        return df\n",
        "\n",
        "    def insert_messages(df: DataFrame, batch_id):\n",
        "        \"\"\"\n",
        "        Called for each micro-batch.\n",
        "        Enriches the DataFrame and writes to Parquet in append mode.\n",
        "        \"\"\"\n",
        "        enrich = enrich_data(df)\n",
        "        enrich.write.mode(\"append\").format(\"parquet\").save(\"content/lake/bronze/messages/data\")\n",
        "\n",
        "    # Create streaming source with a rate of 1 row per second\n",
        "    df_stream = spark.readStream.format(\"rate\").option(\"rowsPerSecond\", 1).load()\n",
        "\n",
        "    # Start streaming with checkpointing enabled for fault-tolerance\n",
        "    query = (df_stream.writeStream\n",
        "             .outputMode('append')\n",
        "             .trigger(processingTime='1 seconds')\n",
        "             .option(\"checkpointLocation\", \"content/lake/bronze/messages/checkpoint\")  # Ensure recovery if interrupted\n",
        "             .foreachBatch(insert_messages)\n",
        "             .start())\n",
        "\n",
        "    print(\"2. Producer streaming for 1 minute as required...\")\n",
        "\n",
        "    # Run for exactly 1 minute as specified\n",
        "    query.awaitTermination(60)\n",
        "\n",
        "    print(\"‚úÖ Producer completed successfully\")\n",
        "\n",
        "    return query\n",
        "\n",
        "# ----------------------------------------\n",
        "# Display results after streaming is done\n",
        "# ----------------------------------------\n",
        "def show_producer_results():\n",
        "    \"\"\"\n",
        "    Reads and displays results from the produced Parquet data.\n",
        "    Shows total records, schema, and a sample of the data.\n",
        "    \"\"\"\n",
        "    print(\"3. Producer Results\")\n",
        "\n",
        "    try:\n",
        "        # Read produced data\n",
        "        produced_data = spark.read.parquet(\"content/lake/bronze/messages/data\")\n",
        "        total_records = produced_data.count()\n",
        "\n",
        "        print(f\"Total records produced: {total_records}\")\n",
        "\n",
        "        # Show schema\n",
        "        print(\"Data Schema:\")\n",
        "        produced_data.printSchema()\n",
        "\n",
        "        # Show sample data\n",
        "        print(\"Sample Data (first 10 records):\")\n",
        "        produced_data.show(10, truncate=False)\n",
        "\n",
        "        return produced_data\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error reading producer results: {e}\")\n",
        "        return None\n",
        "\n",
        "# ----------------------------------------\n",
        "# Resource cleanup\n",
        "# ----------------------------------------\n",
        "def cleanup_producer_resources():\n",
        "    \"\"\"\n",
        "    Stops all active Spark streaming queries to free resources.\n",
        "    Prints a warning if unable to stop any stream.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Stop any active streaming queries\n",
        "        active_streams = spark.streams.active\n",
        "        if active_streams:\n",
        "            for stream in active_streams:\n",
        "                try:\n",
        "                    stream.stop()\n",
        "                except Exception as e:\n",
        "                    print(f\"‚ö†Ô∏è  Warning stopping stream {stream.id}: {e}\")\n",
        "\n",
        "        print(\"Producer cleanup completed\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è  Warning during producer cleanup: {e}\")\n",
        "\n",
        "# =====================================\n",
        "# MAIN EXECUTION: Run the complete step\n",
        "# =====================================\n",
        "def run_producer_step1():\n",
        "    \"\"\"\n",
        "    Executes Step 1 of Challenge 1:\n",
        "    1. Cleans up and prepares directories,\n",
        "    2. Runs the producer for 1 minute,\n",
        "    3. Displays results,\n",
        "    4. Always performs resource cleanup.\n",
        "    \"\"\"\n",
        "    print(\"Executing Challenge 1 - Step 1: Producer\")\n",
        "\n",
        "    try:\n",
        "        # 1. Setup directories\n",
        "        cleanup_and_setup_directories()\n",
        "        print(\"1. Cleanup and Setup Directories\")\n",
        "\n",
        "        # 2. Run updated producer\n",
        "        query = run_updated_producer()\n",
        "\n",
        "        # 3. Show results\n",
        "        produced_data = show_producer_results()\n",
        "\n",
        "        print(\"Step 1 completed successfully!\")\n",
        "\n",
        "        return produced_data\n",
        "\n",
        "    finally:\n",
        "        # Always cleanup\n",
        "        cleanup_producer_resources()"
      ],
      "metadata": {
        "id": "IlOWSfQsqnMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_producer_step1()"
      ],
      "metadata": {
        "id": "b5ztgZqcr7D4",
        "outputId": "8687aa9d-f8b3-4d09-daac-c6776b475a22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Executing Challenge 1 - Step 1: Producer\n",
            "1. Cleanup and Setup Directories\n",
            "2. Producer streaming for 1 minute as required...\n",
            "‚úÖ Producer completed successfully\n",
            "3. Producer Results\n",
            "Total records produced: 59\n",
            "Data Schema:\n",
            "root\n",
            " |-- timestamp: timestamp (nullable = true)\n",
            " |-- value: long (nullable = true)\n",
            " |-- event_type: string (nullable = true)\n",
            " |-- message_id: string (nullable = true)\n",
            " |-- channel: string (nullable = true)\n",
            " |-- country_id: integer (nullable = true)\n",
            " |-- user_id: integer (nullable = true)\n",
            "\n",
            "Sample Data (first 10 records):\n",
            "+-----------------------+-----+----------+------------------------------------+-------+----------+-------+\n",
            "|timestamp              |value|event_type|message_id                          |channel|country_id|user_id|\n",
            "+-----------------------+-----+----------+------------------------------------+-------+----------+-------+\n",
            "|2025-07-13 01:04:27.546|21   |RECEIVED  |1b692875-885d-432d-9e09-9c7d6f6bdca3|EMAIL  |2015      |1038   |\n",
            "|2025-07-13 01:04:35.546|29   |RECEIVED  |4a7c1c3c-7a65-4a30-9261-c365bd2807a4|EMAIL  |2006      |1050   |\n",
            "|2025-07-13 01:04:25.546|19   |RECEIVED  |e3347406-92a7-42ba-b100-8cfc4cccc2ce|EMAIL  |2004      |1015   |\n",
            "|2025-07-13 01:04:17.546|11   |RECEIVED  |f6a9c10c-b23e-468a-b552-f3a017096d27|OTHER  |2006      |1047   |\n",
            "|2025-07-13 01:04:59.546|53   |CREATED   |22f79baa-f315-40c6-85c3-35efa53521d6|EMAIL  |2012      |1026   |\n",
            "|2025-07-13 01:04:28.546|22   |CREATED   |697e2c39-0c25-42e7-950b-97d11dd2edbb|EMAIL  |2011      |1007   |\n",
            "|2025-07-13 01:04:51.546|45   |RECEIVED  |088403c0-92f8-4f5d-b15c-340388bfa233|PUSH   |2006      |1007   |\n",
            "|2025-07-13 01:04:58.546|52   |CLICKED   |0ae1a7fc-0b7c-44c6-b19c-028eac61dfeb|EMAIL  |2011      |1026   |\n",
            "|2025-07-13 01:04:20.546|14   |RECEIVED  |9a0b0d1a-914e-43ea-b3fa-7aed72a7545a|CHAT   |2015      |1002   |\n",
            "|2025-07-13 01:04:23.546|17   |CLICKED   |7edb5e7c-84a2-43ff-bef2-240964aca7f8|OTHER  |2007      |1010   |\n",
            "+-----------------------+-----+----------+------------------------------------+-------+----------+-------+\n",
            "only showing top 10 rows\n",
            "\n",
            "Step 1 completed successfully!\n",
            "Producer cleanup completed\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[timestamp: timestamp, value: bigint, event_type: string, message_id: string, channel: string, country_id: int, user_id: int]"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RraxHCycMdEZ"
      },
      "source": [
        "# Additional datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cfsus3dxMcQI"
      },
      "outputs": [],
      "source": [
        "countries = [\n",
        "    {\"country_id\": 2000, \"country\": \"Brazil\"},\n",
        "    {\"country_id\": 2001, \"country\": \"Portugal\"},\n",
        "    {\"country_id\": 2002, \"country\": \"Spain\"},\n",
        "    {\"country_id\": 2003, \"country\": \"Germany\"},\n",
        "    {\"country_id\": 2004, \"country\": \"France\"},\n",
        "    {\"country_id\": 2005, \"country\": \"Italy\"},\n",
        "    {\"country_id\": 2006, \"country\": \"United Kingdom\"},\n",
        "    {\"country_id\": 2007, \"country\": \"United States\"},\n",
        "    {\"country_id\": 2008, \"country\": \"Canada\"},\n",
        "    {\"country_id\": 2009, \"country\": \"Australia\"},\n",
        "    {\"country_id\": 2010, \"country\": \"Japan\"},\n",
        "    {\"country_id\": 2011, \"country\": \"China\"},\n",
        "    {\"country_id\": 2012, \"country\": \"India\"},\n",
        "    {\"country_id\": 2013, \"country\": \"South Korea\"},\n",
        "    {\"country_id\": 2014, \"country\": \"Russia\"},\n",
        "    {\"country_id\": 2015, \"country\": \"Argentina\"}\n",
        "]\n",
        "\n",
        "countries = spark.createDataFrame(countries)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pg2nx03_Sn62"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# === CHALLENGE 1 - STEP 2 ===\n"
      ],
      "metadata": {
        "id": "cIdz0NG51IKh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swvPj9hVpzNf"
      },
      "source": [
        "# Streaming Messages x Messages Corrupted"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import shutil\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "# =====================================\n",
        "# Countries Dataset Setup\n",
        "# =====================================\n",
        "def create_countries_dataset():\n",
        "    \"\"\"\n",
        "    Create static dataset mapping country_id to country name.\n",
        "    \"\"\"\n",
        "    countries = [\n",
        "        {\"country_id\": 2000, \"country\": \"Brazil\"},\n",
        "        {\"country_id\": 2001, \"country\": \"Portugal\"},\n",
        "        {\"country_id\": 2002, \"country\": \"Spain\"},\n",
        "        {\"country_id\": 2003, \"country\": \"Germany\"},\n",
        "        {\"country_id\": 2004, \"country\": \"France\"},\n",
        "        {\"country_id\": 2005, \"country\": \"Italy\"},\n",
        "        {\"country_id\": 2006, \"country\": \"United Kingdom\"},\n",
        "        {\"country_id\": 2007, \"country\": \"United States\"},\n",
        "        {\"country_id\": 2008, \"country\": \"Canada\"},\n",
        "        {\"country_id\": 2009, \"country\": \"Australia\"},\n",
        "        {\"country_id\": 2010, \"country\": \"Japan\"},\n",
        "        {\"country_id\": 2011, \"country\": \"China\"},\n",
        "        {\"country_id\": 2012, \"country\": \"India\"},\n",
        "        {\"country_id\": 2013, \"country\": \"South Korea\"},\n",
        "        {\"country_id\": 2014, \"country\": \"Russia\"},\n",
        "        {\"country_id\": 2015, \"country\": \"Argentina\"}\n",
        "    ]\n",
        "\n",
        "    countries_df = spark.createDataFrame(countries)\n",
        "    return countries_df\n",
        "\n",
        "# =====================================\n",
        "# Define schema for bronze layer data\n",
        "# =====================================\n",
        "def define_bronze_schema():\n",
        "    \"\"\"\n",
        "    Define the expected schema for messages in the bronze layer.\n",
        "    Ensures consistent parsing of timestamp, IDs, and event attributes.\n",
        "    \"\"\"\n",
        "    return StructType([\n",
        "        StructField(\"timestamp\", TimestampType(), True),\n",
        "        StructField(\"value\", LongType(), True),\n",
        "        StructField(\"event_type\", StringType(), True),\n",
        "        StructField(\"message_id\", StringType(), True),\n",
        "        StructField(\"channel\", StringType(), True),\n",
        "        StructField(\"country_id\", IntegerType(), True),\n",
        "        StructField(\"user_id\", IntegerType(), True)\n",
        "    ])\n",
        "\n",
        "# ---------------------------------------------\n",
        "# Clean up and prepare silver layer directories\n",
        "# ---------------------------------------------\n",
        "def setup_silver_directories():\n",
        "    \"\"\"\n",
        "    Clean up and recreate silver layer directories.\n",
        "    Ensures clean separation of valid and corrupted messages.\n",
        "    \"\"\"\n",
        "    silver_paths = [\n",
        "        \"content/lake/silver/messages\",\n",
        "        \"content/lake/silver/messages_corrupted\",\n",
        "        \"content/lake/silver/etl_checkpoint\"\n",
        "    ]\n",
        "\n",
        "    for path in silver_paths:\n",
        "        if os.path.exists(path):\n",
        "            shutil.rmtree(path)\n",
        "\n",
        "    os.makedirs(\"content/lake/silver/messages/data\", exist_ok=True)\n",
        "    os.makedirs(\"content/lake/silver/messages_corrupted/data\", exist_ok=True)\n",
        "    os.makedirs(\"content/lake/silver/etl_checkpoint/checkpoint\", exist_ok=True)\n",
        "\n",
        "# =====================================\n",
        "# Foreachbatch processing\n",
        "# =====================================\n",
        "def process_batch_to_both_sinks(batch_df, batch_id):\n",
        "    \"\"\"\n",
        "    Split incoming micro-batch into corrupted and clean messages.\n",
        "    Enrich with country name and write each set to its respective silver path.\n",
        "    \"\"\"\n",
        "    if batch_df.isEmpty():\n",
        "        print(f\"üì≠ Batch {batch_id}: No data\")\n",
        "        return\n",
        "\n",
        "    countries_df = create_countries_dataset()\n",
        "\n",
        "    enriched_batch = batch_df.join(\n",
        "        broadcast(countries_df), \"country_id\", \"left\"\n",
        "    ).withColumn(\"country_name\", col(\"country\")).drop(\"country\")\n",
        "\n",
        "    enriched_batch = enriched_batch.withColumn(\n",
        "        \"date\", date_format(col(\"timestamp\"), \"yyyy-MM-dd\")\n",
        "    )\n",
        "\n",
        "    corrupted = enriched_batch.filter(col(\"event_type\").isNull() | (col(\"event_type\") == \"\") | (col(\"event_type\") == \"NONE\"))\n",
        "    clean = enriched_batch.filter(~(col(\"event_type\").isNull() | (col(\"event_type\") == \"\") | (col(\"event_type\") == \"NONE\")))\n",
        "\n",
        "    if not corrupted.isEmpty():\n",
        "        corrupted.write.mode(\"append\").partitionBy(\"date\").parquet(\"content/lake/silver/messages_corrupted/data\")\n",
        "    if not clean.isEmpty():\n",
        "        clean.write.mode(\"append\").partitionBy(\"date\").parquet(\"content/lake/silver/messages/data\")\n",
        "\n",
        "# ----------------------------------------\n",
        "# Streaming query setup and execution\n",
        "# ----------------------------------------\n",
        "def start_single_stream_with_foreach():\n",
        "    \"\"\"\n",
        "    Start structured streaming query using foreachBatch logic.\n",
        "    Reads from bronze and routes to silver based on corruption logic.\n",
        "    \"\"\"\n",
        "    setup_silver_directories()\n",
        "\n",
        "    bronze_schema = define_bronze_schema()\n",
        "\n",
        "    bronze_stream = spark.readStream \\\n",
        "        .format(\"parquet\") \\\n",
        "        .schema(bronze_schema) \\\n",
        "        .load(\"content/lake/bronze/messages/data\")\n",
        "\n",
        "    return bronze_stream.writeStream \\\n",
        "        .foreachBatch(process_batch_to_both_sinks) \\\n",
        "        .option(\"checkpointLocation\", \"content/lake/silver/etl_checkpoint/checkpoint\") \\\n",
        "        .trigger(processingTime=\"5 seconds\") \\\n",
        "        .start()\n",
        "\n",
        "# ----------------------------------------\n",
        "# Resource cleanup\n",
        "# ----------------------------------------\n",
        "def cleanup_etl_resources():\n",
        "    \"\"\"\n",
        "    Stops all active Spark streaming queries to free resources.\n",
        "    Prints a warning if unable to stop any stream.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        active_streams = spark.streams.active\n",
        "        if active_streams:\n",
        "            for stream in active_streams:\n",
        "                try:\n",
        "                    stream.stop()\n",
        "                except Exception as e:\n",
        "                    print(f\"‚ö†Ô∏è  Warning stopping stream {stream.id}: {e}\")\n",
        "        print(\"ETL cleanup completed\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è  Warning during ETL cleanup: {e}\")\n",
        "\n",
        "# =====================================\n",
        "# MAIN EXECUTION\n",
        "# =====================================\n",
        "def run_etl_step2(run_time=20):\n",
        "    \"\"\"\n",
        "    Executes Step 2 of Challenge 1: Main execution for Bronze to Silver ETL pipeline using foreachBatch.\n",
        "    Only runs the streaming process and stops it after run_time seconds.\n",
        "    \"\"\"\n",
        "    print(\"Executing Challenge 1 - Step 2: Bronze to Silver Layer\")\n",
        "\n",
        "    try:\n",
        "        query = start_single_stream_with_foreach()\n",
        "        time.sleep(run_time)\n",
        "        query.stop()\n",
        "        print(\"‚úÖ Stream stopped successfully after ETL run\")\n",
        "\n",
        "        # Quick status\n",
        "        try:\n",
        "            clean_count = spark.read.parquet(\"content/lake/silver/messages/data\").count()\n",
        "            corrupted_count = spark.read.parquet(\"content/lake/silver/messages_corrupted/data\").count()\n",
        "            print(f\"Quick results: {clean_count} clean, {corrupted_count} corrupted\")\n",
        "        except:\n",
        "            print(\"Data still processing or not available yet\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå ETL failed: {e}\")\n",
        "        raise e\n",
        "    finally:\n",
        "        cleanup_etl_resources()"
      ],
      "metadata": {
        "id": "6Ffthm1PQBGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_etl_step2()"
      ],
      "metadata": {
        "id": "OGbeiGPBSGML",
        "outputId": "93edb24d-209f-44e2-bc18-4af5abf6964a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Executing Challenge 1 - Step 2: Bronze to Silver Layer\n",
            "‚úÖ Stream stopped successfully after ETL run\n",
            "Quick results: 43 clean, 16 corrupted\n",
            "ETL cleanup completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLK9jpjCu3xE"
      },
      "source": [
        "## Checking data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nk8seEvbmvcU"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# ----------------------------\n",
        "# Count and compare all layers\n",
        "# ----------------------------\n",
        "def validate_counts():\n",
        "    \"\"\"\n",
        "    Compare bronze layer record count with total of silver clean + corrupted.\n",
        "    Return summary with counts and validation result.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        bronze_count = spark.read.parquet(\"content/lake/bronze/messages/data\").count()\n",
        "    except:\n",
        "        print(\"‚ùå Error reading bronze layer\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        clean_count = spark.read.parquet(\"content/lake/silver/messages/data\").count()\n",
        "    except:\n",
        "        clean_count = 0\n",
        "\n",
        "    try:\n",
        "        corrupted_count = spark.read.parquet(\"content/lake/silver/messages_corrupted/data\").count()\n",
        "    except:\n",
        "        corrupted_count = 0\n",
        "\n",
        "    silver_total = clean_count + corrupted_count\n",
        "    result = {\n",
        "        \"bronze\": bronze_count,\n",
        "        \"clean\": clean_count,\n",
        "        \"corrupted\": corrupted_count,\n",
        "        \"silver_total\": silver_total,\n",
        "        \"match\": bronze_count == silver_total\n",
        "    }\n",
        "\n",
        "    print(\"\\nüìä COUNT SUMMARY\")\n",
        "    print(f\"Bronze:           {bronze_count}\")\n",
        "    print(f\"Silver Clean:     {clean_count}\")\n",
        "    print(f\"Silver Corrupted: {corrupted_count}\")\n",
        "    print(f\"Silver Total:     {silver_total}\")\n",
        "    print(f\"Match:            {'‚úÖ YES' if result['match'] else '‚ùå NO'}\")\n",
        "\n",
        "    return result\n",
        "\n",
        "# ----------------------------\n",
        "# Show 5 sample records\n",
        "# ----------------------------\n",
        "def show_samples():\n",
        "    \"\"\"Display 5 records from each layer for quick inspection.\"\"\"\n",
        "    try:\n",
        "        print(\"\\nüü¶ Bronze Sample:\")\n",
        "        spark.read.parquet(\"content/lake/bronze/messages/data\").show(5, truncate=False)\n",
        "    except:\n",
        "        print(\"No bronze data found\")\n",
        "\n",
        "    try:\n",
        "        print(\"\\nüü¢ Silver Clean Sample:\")\n",
        "        spark.read.parquet(\"content/lake/silver/messages/data\").show(5, truncate=False)\n",
        "    except:\n",
        "        print(\"No clean data found\")\n",
        "\n",
        "    try:\n",
        "        print(\"\\nüî¥ Silver Corrupted Sample:\")\n",
        "        spark.read.parquet(\"content/lake/silver/messages_corrupted/data\").show(5, truncate=False)\n",
        "    except:\n",
        "        print(\"No corrupted data found\")\n",
        "\n",
        "# ----------------------------\n",
        "# Validate Results\n",
        "# ----------------------------\n",
        "def validate_results():\n",
        "    \"\"\"Run validation.\"\"\"\n",
        "    result = validate_counts()\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "validate_results()"
      ],
      "metadata": {
        "id": "VpSlHqF_B1dZ",
        "outputId": "04a76476-868f-40c8-b4d8-615418a6c127",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä COUNT SUMMARY\n",
            "Bronze:           59\n",
            "Silver Clean:     43\n",
            "Silver Corrupted: 16\n",
            "Silver Total:     59\n",
            "Match:            ‚úÖ YES\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'bronze': 59, 'clean': 43, 'corrupted': 16, 'silver_total': 59, 'match': True}"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfxIlBISSvRP"
      },
      "source": [
        "# Challenge 2\n",
        "\n",
        "- Run business report\n",
        "- But first, there is a bug in the system which is causing some duplicated messages, we need to exclude these lines from the report\n",
        "\n",
        "- removing duplicates logic:\n",
        "  - Identify possible duplicates on message_id, event_type and channel\n",
        "  - in case of duplicates, consider only the first message (occurrence by timestamp)\n",
        "  - Ex:\n",
        "    In table below, the correct message to consider is the second line\n",
        "\n",
        "```\n",
        "    message_id | channel | event_type | timestamp\n",
        "    123        | CHAT    | CREATED    | 10:10:01\n",
        "    123        | CHAT    | CREATED    | 07:56:45 (first occurrence)\n",
        "    123        | CHAT    | CREATED    | 08:13:33\n",
        "```\n",
        "\n",
        "- After cleaning the data we're able to create the busines report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R3J9XyOHhqvU"
      },
      "outputs": [],
      "source": [
        "# dedup data\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.window import Window\n",
        "df = spark.read.format(\"parquet\").load(\"content/lake/silver/messages\")\n",
        "dedup = df.withColumn(\"row_number\", F.row_number().over(Window.partitionBy(\"message_id\", \"event_type\", \"channel\").orderBy(\"timestamp\"))).filter(\"row_number = 1\").drop(\"row_number\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RF9L9i25lk74"
      },
      "source": [
        "### Report 1\n",
        "  - Aggregate data by date, event_type and channel\n",
        "  - Count number of messages\n",
        "  - pivot event_type from rows into columns\n",
        "  - schema expected:\n",
        "  \n",
        "```\n",
        "|      date|channel|CLICKED|CREATED|OPEN|RECEIVED|SENT|\n",
        "+----------+-------+-------+-------+----+--------+----+\n",
        "|2024-12-03|    SMS|      4|      4|   1|       1|   5|\n",
        "|2024-12-03|   CHAT|      3|      7|   5|       8|   4|\n",
        "|2024-12-03|   PUSH|   NULL|      3|   4|       3|   4|\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UPHSMSXnTKgu",
        "outputId": "6f111e40-6f99-4107-8090-db8525645745",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------+-------+-------+----+--------+----+\n",
            "|date      |channel|CLICKED|CREATED|OPEN|RECEIVED|SENT|\n",
            "+----------+-------+-------+-------+----+--------+----+\n",
            "|2025-07-13|CHAT   |1      |2      |1   |2       |4   |\n",
            "|2025-07-13|EMAIL  |2      |2      |1   |3       |1   |\n",
            "|2025-07-13|OTHER  |1      |3      |1   |1       |NULL|\n",
            "|2025-07-13|PUSH   |2      |NULL   |1   |2       |4   |\n",
            "|2025-07-13|SMS    |NULL   |2      |2   |3       |1   |\n",
            "+----------+-------+-------+-------+----+--------+----+\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[date: date, channel: string, CLICKED: bigint, CREATED: bigint, OPEN: bigint, RECEIVED: bigint, SENT: bigint]"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ],
      "source": [
        "# report 1\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "spark = SparkSession.getActiveSession() or SparkSession.builder.getOrCreate()\n",
        "\n",
        "# ----------------------------------------\n",
        "# Remove duplicate messages based on logic\n",
        "# ----------------------------------------\n",
        "def remove_duplicates():\n",
        "    \"\"\"\n",
        "    Remove duplicate messages based on message_id, event_type, and channel.\n",
        "    Keep only the earliest timestamp per group.\n",
        "    \"\"\"\n",
        "    df = spark.read.format(\"parquet\").load(\"content/lake/silver/messages\")\n",
        "\n",
        "    window_spec = Window.partitionBy(\"message_id\", \"event_type\", \"channel\").orderBy(\"timestamp\")\n",
        "\n",
        "    dedup = df.withColumn(\"row_number\", F.row_number().over(window_spec)).filter(F.col(\"row_number\") == 1).drop(\"row_number\")\n",
        "\n",
        "    return dedup\n",
        "\n",
        "# ----------------------------------------\n",
        "# Generate pivoted report 1\n",
        "# ----------------------------------------\n",
        "def generate_report1():\n",
        "    \"\"\"\n",
        "    Create pivot report grouped by date and channel with event_type columns.\n",
        "    \"\"\"\n",
        "    dedup = remove_duplicates()\n",
        "\n",
        "    report = dedup.groupBy(\"date\", \"channel\") \\\n",
        "                 .pivot(\"event_type\", [\"CLICKED\", \"CREATED\", \"OPEN\", \"RECEIVED\", \"SENT\"]) \\\n",
        "                 .agg(F.count(\"message_id\")) \\\n",
        "                 .orderBy(\"date\", \"channel\")\n",
        "\n",
        "    report.show(truncate=False)\n",
        "    return report\n",
        "\n",
        "generate_report1()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxwOawo2lwQH"
      },
      "source": [
        "## Report 2\n",
        "\n",
        "- Identify the most active users by channel (sorted by number of iterations)\n",
        "- schema expected:\n",
        "\n",
        "```\n",
        "+-------+----------+----+-----+-----+----+---+\n",
        "|user_id|iterations|CHAT|EMAIL|OTHER|PUSH|SMS|\n",
        "+-------+----------+----+-----+-----+----+---+\n",
        "|   1022|         5|   2|    0|    1|   0|  2|\n",
        "|   1004|         4|   1|    1|    1|   1|  0|\n",
        "|   1013|         4|   0|    0|    2|   1|  1|\n",
        "|   1020|         4|   2|    0|    1|   1|  0|\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rsS7bkAJmWsW",
        "outputId": "66c10b1d-6a7b-45ac-828c-4c57e5ad9e86",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+----+-----+-----+----+---+\n",
            "|user_id|iterations|CHAT|EMAIL|OTHER|PUSH|SMS|\n",
            "+-------+----------+----+-----+-----+----+---+\n",
            "|1019   |3         |0   |0    |1    |1   |1  |\n",
            "|1047   |2         |1   |0    |1    |0   |0  |\n",
            "|1026   |2         |0   |2    |0    |0   |0  |\n",
            "|1002   |2         |1   |0    |1    |0   |0  |\n",
            "|1045   |2         |0   |1    |1    |0   |0  |\n",
            "|1007   |2         |0   |1    |0    |1   |0  |\n",
            "|1049   |2         |1   |0    |0    |1   |0  |\n",
            "|1003   |2         |0   |0    |0    |1   |1  |\n",
            "|1009   |2         |0   |0    |1    |0   |1  |\n",
            "|1025   |1         |0   |0    |0    |0   |1  |\n",
            "|1016   |1         |0   |0    |0    |0   |1  |\n",
            "|1005   |1         |0   |0    |0    |0   |1  |\n",
            "|1030   |1         |1   |0    |0    |0   |0  |\n",
            "|1021   |1         |1   |0    |0    |0   |0  |\n",
            "|1028   |1         |1   |0    |0    |0   |0  |\n",
            "|1032   |1         |0   |0    |0    |0   |1  |\n",
            "|1010   |1         |0   |0    |1    |0   |0  |\n",
            "|1048   |1         |0   |1    |0    |0   |0  |\n",
            "|1050   |1         |0   |1    |0    |0   |0  |\n",
            "|1035   |1         |0   |0    |0    |1   |0  |\n",
            "+-------+----------+----+-----+-----+----+---+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[user_id: int, iterations: bigint, CHAT: bigint, EMAIL: bigint, OTHER: bigint, PUSH: bigint, SMS: bigint]"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ],
      "source": [
        "# report 2\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.window import Window\n",
        "from functools import reduce\n",
        "from operator import add\n",
        "\n",
        "spark = SparkSession.getActiveSession() or SparkSession.builder.getOrCreate()\n",
        "\n",
        "# ----------------------------------------\n",
        "# Remove duplicate messages based on logic\n",
        "# ----------------------------------------\n",
        "def remove_duplicates():\n",
        "    \"\"\"\n",
        "    Remove duplicate messages based on message_id, event_type, and channel.\n",
        "    Keep only the earliest timestamp per group.\n",
        "    \"\"\"\n",
        "    df = spark.read.format(\"parquet\").load(\"content/lake/silver/messages\")\n",
        "\n",
        "    window_spec = Window.partitionBy(\"message_id\", \"event_type\", \"channel\").orderBy(\"timestamp\")\n",
        "\n",
        "    dedup = df.withColumn(\"row_number\", F.row_number().over(window_spec)).filter(F.col(\"row_number\") == 1).drop(\"row_number\")\n",
        "\n",
        "    return dedup\n",
        "\n",
        "# -----------------------------------------------\n",
        "# Generate report of most active users by channel\n",
        "# -----------------------------------------------\n",
        "def generate_report2():\n",
        "    \"\"\"\n",
        "    Identify the most active users by channel.\n",
        "    Output includes user_id, total message count, and breakdown by channel.\n",
        "    \"\"\"\n",
        "    dedup = remove_duplicates()\n",
        "\n",
        "    user_activity = dedup.groupBy(\"user_id\", \"channel\") \\\n",
        "                          .agg(F.count(\"message_id\").alias(\"count\"))\n",
        "\n",
        "    pivoted = user_activity.groupBy(\"user_id\") \\\n",
        "                            .pivot(\"channel\", [\"CHAT\", \"EMAIL\", \"OTHER\", \"PUSH\", \"SMS\"]) \\\n",
        "                            .agg(F.first(\"count\"))\n",
        "\n",
        "    channel_cols = [\"CHAT\", \"EMAIL\", \"OTHER\", \"PUSH\", \"SMS\"]\n",
        "\n",
        "    for c in channel_cols:\n",
        "        pivoted = pivoted.withColumn(c, F.coalesce(F.col(c), F.lit(0)))\n",
        "\n",
        "    result = pivoted.withColumn(\n",
        "        \"iterations\",\n",
        "        reduce(add, [F.col(c) for c in channel_cols])\n",
        "    ).select(\"user_id\", \"iterations\", *channel_cols).orderBy(F.col(\"iterations\").desc())\n",
        "\n",
        "    result.show(truncate=False)\n",
        "    return result\n",
        "\n",
        "generate_report2()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9_kzDbDwDOS"
      },
      "source": [
        "# Challenge 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ef0RjFTxwE5y"
      },
      "outputs": [],
      "source": [
        "# Theoretical question:\n",
        "\n",
        "# A new usecase requires the message data to be aggregate in near real time\n",
        "# They want to build a dashboard embedded in the platform website to analyze message data in low latency (few minutes)\n",
        "# This application will access directly the data aggregated by streaming process\n",
        "\n",
        "# Q1:\n",
        "- What would be your suggestion to achieve that using Spark Structure Streaming?\n",
        "Or would you choose a different data processing tool?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spark Structured Streaming stands out by supporting continuous data processing and guaranteeing exactly-once semantics, thereby avoiding data duplication or event loss, even in failure scenarios. It ensures fault tolerance with automatic recovery from failures through checkpointing. It is used for advanced aggregations and analytical operations, and, thanks to its distributed architecture, it offers scalability ‚Äì enabling it to handle growing data volumes and multiple pipelines simultaneously. Another significant advantage is its maturity. This technology offers extensive documentation, a strong support community and it has native integration with a wide range of sources and destinations, such as Kafka.\n",
        "\n",
        "As was mentioned before, there are several options for near real-time streaming such as Apache Flink. This technology offers lower latency than Spark for event-time processing, making it more suitable for use cases that require highly precise event handling and complex event processing patterns. Another advantage is that it offers more accurate watermark handling. On the other hand, Flink has a steeper learning curve than Spark, it is generally more complex to set up and maintain and compared to Spark it features a smaller ecosystem.\n",
        "\n",
        "Kafka Streams is another viable option, especially for applications that require lightweight processing. It is well-suited for microservices architectures and offers simple deployment. However, compared to Spark, Kafka Streams doesn‚Äôt scale as effectively, is less suitable for complex analytics and being a JVM-only solution, it offers less flexibility in heterogeneous environments.\n",
        "\n",
        "In conclusion, Spark Structured Streaming is the optimal choice for implementing a near real-time dashboard. It offers the right balance between performance, scalability, reliability, and maintainability. The platform supports sub-minute latency for dashboard updates, handles growing message volumes efficiently, provides robust fault tolerance with exactly-once processing, and benefits from an open-source model with wide industry adoption. The requirement for updates with a latency of a few minutes is easily achievable using processing triggers and appropriate configurations, making Spark Structured Streaming the most practical and future-proof solution for this scenario.\n"
      ],
      "metadata": {
        "id": "OCbysTNgwOpR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q2:\n",
        "- Which storage would you use and why? (database?, data lake?, kafka?)"
      ],
      "metadata": {
        "id": "0DOatOQkwWI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the context of near real-time analytics, selecting a storage solution to serve aggregated data to a dashboard is a critical architectural decision. After an evaluation of industry practices and technological alternatives, a hybrid architecture - combining the strengths of Apache Kafka, Delta Lake, and Redis - emerges as the most robust and future-proof approach.\n",
        "\n",
        "The Ingestion Layer is guaranteed by Apache Kafka, due to its proven capacity for handling large streams of data and for its high throughput. It provides robust and durable storage with configurable retention. This technology is suitable for parallel processing through partitioning, and exactly-once delivery semantics. For these reasons, Kafka is a solid choice for bringing message streams into the analytical environment, ensuring that data transport remains both reliable and scalable.\n",
        "\n",
        "At the analytical storage layer, Delta Lake ensures ACID-compliant transactions and supports schema evolution. Its integration with Spark Structured Streaming unifies both batch and streaming processing, providing the flexibility required for both real-time and retrospective analytics. Delta Lake's optimization further enhances query performance, especially when partitioned by date or other dimensions important for dashboard reporting.\n",
        "\n",
        "To deliver quick responses for the dashboard, Redis acts as a high-performance in-memory cache that holds the most recent aggregates. This in-memory storage ensures sub-millisecond response times for frequent queries, supports automatic data expiration, and can leverage Pub/Sub features for real-time updates. By keeping the most relevant or recently used data in memory, Redis minimizes resource utilization while maximizing end-user experience.\n",
        "\n",
        "This hybrid architecture is designed for seamless scalability. Both Kafka and Delta Lake allow for horizontal partitioning, so the system can effortlessly handle increases in data volume and concurrent users. Redis can be sharded as needed to support frequent dashboard queries, ensuring consistent performance even as demand grows. Cost efficiency is maintained by keeping only essential, frequently accessed data in memory through Redis, carefully optimizing how data is stored in Delta Lake, and configuring Kafka‚Äôs retention policies according to business needs.\n",
        "\n",
        "\n",
        "\n",
        "Raw Messages ‚Üí Kafka ‚Üí Spark Structured Streaming ‚Üí Delta Lake\n",
        "                                    ‚Üì\n",
        "                              Redis Cache ‚Üê Dashboard API\n",
        "                                    ‚Üì\n",
        "                            Web Dashboard\n",
        "\n",
        "The architectural data flow can be summarized as follows:\n",
        "Raw messages are ingested by Kafka, processed and aggregated in near real-time by Spark Structured Streaming, persisted in Delta Lake for analytical and historical use, and simultaneously cached in Redis to serve the dashboard with immediate access to up-to-date metrics.\n",
        "\n",
        "The combination of Kafka, Delta Lake, and Redis fully addresses the main requirements for serving aggregated data to a real-time dashboard. Redis delivers low latency, allowing the dashboard to respond quickly to user queries. Kafka provides the high throughput and durability needed to process streaming data at scale. Delta Lake ensures robust analytics and data consistency, supporting advanced aggregations and historical analysis. By leveraging mature, open-source technologies in roles to which they are best suited, this solution achieves operational reliability and cost-effectiveness.\n",
        "This architecture remains flexible, easily adapting to evolving business needs, supporting new metrics, and accommodating increasingly complex analytical demands.\n",
        "In conclusion, this design is well aligned with both current best practices in real-time data architectures and the practical needs of modern dashboard systems.\n",
        "\n"
      ],
      "metadata": {
        "id": "jwfv30ntv-0v"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}