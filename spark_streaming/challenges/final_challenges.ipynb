{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/margaridagomes/dataeng-basic-course/blob/main/spark_streaming/challenges/final_challenges.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_GBE9UsyxwK"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9LeYFsPTjAb"
      },
      "source": [
        "# Setting up PySpark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYXeODL0T1fO",
        "outputId": "497c35ac-6d8b-48a2-a30c-63ce58f66829"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "%pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rcybt71kTDNt"
      },
      "source": [
        "# Context\n",
        "Message events are coming from platform message broker (kafka, pubsub, kinesis...).\n",
        "You need to process the data according to the requirements.\n",
        "\n",
        "Message schema:\n",
        "- timestamp\n",
        "- value\n",
        "- event_type\n",
        "- message_id\n",
        "- country_id\n",
        "- user_id\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkyPORKNSYvV"
      },
      "source": [
        "# Challenge 1\n",
        "\n",
        "Step 1\n",
        "- Change exising producer\n",
        "\t- Change parquet location to \"/content/lake/bronze/messages/data\"\n",
        "\t- Add checkpoint (/content/lake/bronze/messages/checkpoint)\n",
        "\t- Delete /content/lake/bronze/messages and reprocess data\n",
        "\t- For reprocessing, run the streaming for at least 1 minute, then stop it\n",
        "\n",
        "Step 2\n",
        "- Implement new stream job to read from messages in bronze layer and split result in two locations\n",
        "\t- \"messages_corrupted\"\n",
        "\t\t- logic: event_status is null, empty or equal to \"NONE\"\n",
        "    - extra logic: add country name by joining message with countries dataset\n",
        "\t\t- partition by \"date\" -extract it from timestamp\n",
        "\t\t- location: /content/lake/silver/messages_corrupted/data\n",
        "\n",
        "\t- \"messages\"\n",
        "\t\t- logic: not corrupted data\n",
        "\t\t- extra logic: add country name by joining message with countries dataset\n",
        "\t\t- partition by \"date\" -extract it from timestamp\n",
        "\t\t- location: /content/lake/silver/messages/data\n",
        "\n",
        "\t- technical requirements\n",
        "\t\t- add checkpint (choose location)\n",
        "\t\t- use StructSchema\n",
        "\t\t- Set trigger interval to 5 seconds\n",
        "\t\t- run streaming for at least 20 seconds, then stop it\n",
        "\n",
        "\t- alternatives\n",
        "\t\t- implementing single streaming job with foreach/- foreachBatch logic to write into two locations\n",
        "\t\t- implementing two streaming jobs, one for messages and another for messages_corrupted\n",
        "\t\t- (paying attention on the paths and checkpoints)\n",
        "\n",
        "\n",
        "  - Check results:\n",
        "    - results from messages in bronze layer should match with the sum of messages+messages_corrupted in the silver layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Udk3tohSaXOH",
        "outputId": "57d7413a-ddc4-4cf9-c748-ba7c58769eb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faker\n",
            "  Downloading faker-37.4.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: tzdata in /usr/local/lib/python3.11/dist-packages (from faker) (2025.2)\n",
            "Downloading faker-37.4.0-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faker\n",
            "Successfully installed faker-37.4.0\n"
          ]
        }
      ],
      "source": [
        "%pip install faker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDGMKwBdi1qy"
      },
      "source": [
        "# Producer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tPCOdivrfhYh",
        "outputId": "d64c5b30-ab8c-4fdb-e5bf-7eeeea967874"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 148,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql import DataFrame\n",
        "from faker import Faker\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName('Test streaming').getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "\n",
        "fake = Faker()\n",
        "messages = [fake.uuid4() for _ in range(50)]\n",
        "\n",
        "def enrich_data(df, messages=messages):\n",
        "  fake = Faker()\n",
        "  new_columns = {\n",
        "      'event_type': F.lit(fake.random_element(elements=('OPEN', 'RECEIVED', 'SENT', 'CREATED', 'CLICKED', '', 'NONE'))),\n",
        "      'message_id': F.lit(fake.random_element(elements=messages)),\n",
        "      'channel': F.lit(fake.random_element(elements=('CHAT', 'EMAIL', 'SMS', 'PUSH', 'OTHER'))),\n",
        "      'country_id': F.lit(fake.random_int(min=2000, max=2015)),\n",
        "      'user_id': F.lit(fake.random_int(min=1000, max=1050)),\n",
        "  }\n",
        "  df = df.withColumns(new_columns)\n",
        "  return df\n",
        "\n",
        "def insert_messages(df: DataFrame, batch_id):\n",
        "  enrich = enrich_data(df)\n",
        "  enrich.write.mode(\"append\").format(\"parquet\").save(\"content/lake/bronze/messages\")\n",
        "\n",
        "# read stream\n",
        "df_stream = spark.readStream.format(\"rate\").option(\"rowsPerSecond\", 1).load()\n",
        "\n",
        "# write stream\n",
        "query = (df_stream.writeStream\n",
        ".outputMode('append')\n",
        ".trigger(processingTime='1 seconds')\n",
        ".foreachBatch(insert_messages)\n",
        ".start()\n",
        ")\n",
        "\n",
        "query.awaitTermination(60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KNyUK3yplDhg"
      },
      "outputs": [],
      "source": [
        "query.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZWQExsnzlMFe"
      },
      "outputs": [],
      "source": [
        "df = spark.read.format(\"parquet\").load(\"content/lake/bronze/messages/*\")\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# === CHALLENGE 1 - STEP 1: PRODUCER ==="
      ],
      "metadata": {
        "id": "etGr5pBsqoCf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql import DataFrame, SparkSession\n",
        "from faker import Faker\n",
        "\n",
        "# ----------------------------------------\n",
        "# Spark session initialization and config\n",
        "# ----------------------------------------\n",
        "def validate_and_initialize_spark():\n",
        "    \"\"\"\n",
        "    Validate and initialize Spark session with proper configuration.\n",
        "    If a Spark session is already active, reuse it.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Check if Spark session already exists\n",
        "        existing_spark = SparkSession.getActiveSession()\n",
        "\n",
        "        if existing_spark:\n",
        "            spark = existing_spark\n",
        "        else:\n",
        "            # Initialize Spark Session with settings for streaming\n",
        "            spark = SparkSession.builder.appName(\"StreamingETLProducer\").getOrCreate()\n",
        "\n",
        "        return spark\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Spark session validation failed: {e}\")\n",
        "        raise e\n",
        "\n",
        "# Initialize and validate Spark session\n",
        "spark = validate_and_initialize_spark()\n",
        "sc = spark.sparkContext\n",
        "\n",
        "# ----------------------------------------\n",
        "# Clean up and prepare data directories\n",
        "# ----------------------------------------\n",
        "def cleanup_and_setup_directories():\n",
        "    \"\"\"\n",
        "    Clean up existing directories and create a fresh structure for the bronze/messages layer.\n",
        "    Removes any previous run's data and ensures a clean slate for new data.\n",
        "    \"\"\"\n",
        "    base_dir = \"content/lake/bronze/messages\"\n",
        "\n",
        "    # Remove existing bronze directory as required\n",
        "    if os.path.exists(base_dir):\n",
        "        shutil.rmtree(base_dir)\n",
        "\n",
        "    # Create new directory structure with updated paths\n",
        "    os.makedirs(os.path.join(base_dir, \"data\"), exist_ok=True)\n",
        "    os.makedirs(os.path.join(base_dir, \"checkpoint\"), exist_ok=True)\n",
        "\n",
        "# ----------------------------------------\n",
        "# Producer: Generate and stream fake data\n",
        "# ----------------------------------------\n",
        "def run_updated_producer():\n",
        "    \"\"\"\n",
        "    Run the streaming producer with new configuration.\n",
        "    Generates fake message events and writes them in micro-batches to Parquet files using Spark Structured Streaming.\n",
        "    Runs for 1 minute.\n",
        "    \"\"\"\n",
        "    # Generate fake messages (as in original code)\n",
        "    fake = Faker()\n",
        "    messages = [fake.uuid4() for _ in range(50)]\n",
        "\n",
        "    def enrich_data(df, messages=messages):\n",
        "        \"\"\"\n",
        "        Enriches a DataFrame with random event metadata using Faker.\n",
        "        Adds event_type, message_id, channel, country_id, and user_id.\n",
        "        \"\"\"\n",
        "        fake = Faker()\n",
        "        new_columns = {\n",
        "            'event_type': F.lit(fake.random_element(elements=('OPEN', 'RECEIVED', 'SENT', 'CREATED', 'CLICKED', '', 'NONE'))),\n",
        "            'message_id': F.lit(fake.random_element(elements=messages)),\n",
        "            'channel': F.lit(fake.random_element(elements=('CHAT', 'EMAIL', 'SMS', 'PUSH', 'OTHER'))),\n",
        "            'country_id': F.lit(fake.random_int(min=2000, max=2015)),\n",
        "            'user_id': F.lit(fake.random_int(min=1000, max=1050)),\n",
        "        }\n",
        "        df = df.withColumns(new_columns)\n",
        "        return df\n",
        "\n",
        "    def insert_messages(df: DataFrame, batch_id):\n",
        "        \"\"\"\n",
        "        Called for each micro-batch.\n",
        "        Enriches the DataFrame and writes to Parquet in append mode.\n",
        "        \"\"\"\n",
        "        enrich = enrich_data(df)\n",
        "        enrich.write.mode(\"append\").format(\"parquet\").save(\"content/lake/bronze/messages/data\")\n",
        "\n",
        "    # Create streaming source with a rate of 1 row per second\n",
        "    df_stream = spark.readStream.format(\"rate\").option(\"rowsPerSecond\", 1).load()\n",
        "\n",
        "    # Start streaming with checkpointing enabled for fault-tolerance\n",
        "    query = (df_stream.writeStream\n",
        "             .outputMode('append')\n",
        "             .trigger(processingTime='1 seconds')\n",
        "             .option(\"checkpointLocation\", \"content/lake/bronze/messages/checkpoint\")  # Ensure recovery if interrupted\n",
        "             .foreachBatch(insert_messages)\n",
        "             .start())\n",
        "\n",
        "    print(\"2. Producer streaming for 1 minute as required...\")\n",
        "\n",
        "    # Run for exactly 1 minute as specified\n",
        "    query.awaitTermination(60)\n",
        "\n",
        "    print(\"‚úÖ Producer completed successfully\")\n",
        "\n",
        "    return query\n",
        "\n",
        "# ----------------------------------------\n",
        "# Display results after streaming is done\n",
        "# ----------------------------------------\n",
        "def show_producer_results():\n",
        "    \"\"\"\n",
        "    Reads and displays results from the produced Parquet data.\n",
        "    Shows total records, schema, and a sample of the data.\n",
        "    \"\"\"\n",
        "    print(\"3. Producer Results\")\n",
        "\n",
        "    try:\n",
        "        # Read produced data\n",
        "        produced_data = spark.read.parquet(\"content/lake/bronze/messages/data\")\n",
        "        total_records = produced_data.count()\n",
        "\n",
        "        print(f\"Total records produced: {total_records}\")\n",
        "\n",
        "        # Show schema\n",
        "        print(\"Data Schema:\")\n",
        "        produced_data.printSchema()\n",
        "\n",
        "        # Show sample data\n",
        "        print(\"Sample Data (first 10 records):\")\n",
        "        produced_data.show(10, truncate=False)\n",
        "\n",
        "        return produced_data\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error reading producer results: {e}\")\n",
        "        return None\n",
        "\n",
        "# ----------------------------------------\n",
        "# Resource cleanup\n",
        "# ----------------------------------------\n",
        "def cleanup_producer_resources():\n",
        "    \"\"\"\n",
        "    Stops all active Spark streaming queries to free resources.\n",
        "    Prints a warning if unable to stop any stream.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Stop any active streaming queries\n",
        "        active_streams = spark.streams.active\n",
        "        if active_streams:\n",
        "            for stream in active_streams:\n",
        "                try:\n",
        "                    stream.stop()\n",
        "                except Exception as e:\n",
        "                    print(f\"‚ö†Ô∏è  Warning stopping stream {stream.id}: {e}\")\n",
        "\n",
        "        print(\"Producer cleanup completed\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è  Warning during producer cleanup: {e}\")\n",
        "\n",
        "# =====================================\n",
        "# MAIN EXECUTION: Run the complete step\n",
        "# =====================================\n",
        "def run_producer_step1():\n",
        "    \"\"\"\n",
        "    Executes Step 1 of Challenge 1:\n",
        "    1. Cleans up and prepares directories,\n",
        "    2. Runs the producer for 1 minute,\n",
        "    3. Displays results,\n",
        "    4. Always performs resource cleanup.\n",
        "    \"\"\"\n",
        "    print(\"Executing Challenge 1 - Step 1: Producer\")\n",
        "\n",
        "    try:\n",
        "        # 1. Setup directories\n",
        "        cleanup_and_setup_directories()\n",
        "        print(\"1. Cleanup and Setup Directories\")\n",
        "\n",
        "        # 2. Run updated producer\n",
        "        query = run_updated_producer()\n",
        "\n",
        "        # 3. Show results\n",
        "        produced_data = show_producer_results()\n",
        "\n",
        "        print(\"Step 1 completed successfully!\")\n",
        "\n",
        "        return produced_data\n",
        "\n",
        "    finally:\n",
        "        # Always cleanup\n",
        "        cleanup_producer_resources()"
      ],
      "metadata": {
        "id": "IlOWSfQsqnMo"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_producer_step1()"
      ],
      "metadata": {
        "id": "b5ztgZqcr7D4",
        "outputId": "8687aa9d-f8b3-4d09-daac-c6776b475a22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Executing Challenge 1 - Step 1: Producer\n",
            "1. Cleanup and Setup Directories\n",
            "2. Producer streaming for 1 minute as required...\n",
            "‚úÖ Producer completed successfully\n",
            "3. Producer Results\n",
            "Total records produced: 59\n",
            "Data Schema:\n",
            "root\n",
            " |-- timestamp: timestamp (nullable = true)\n",
            " |-- value: long (nullable = true)\n",
            " |-- event_type: string (nullable = true)\n",
            " |-- message_id: string (nullable = true)\n",
            " |-- channel: string (nullable = true)\n",
            " |-- country_id: integer (nullable = true)\n",
            " |-- user_id: integer (nullable = true)\n",
            "\n",
            "Sample Data (first 10 records):\n",
            "+-----------------------+-----+----------+------------------------------------+-------+----------+-------+\n",
            "|timestamp              |value|event_type|message_id                          |channel|country_id|user_id|\n",
            "+-----------------------+-----+----------+------------------------------------+-------+----------+-------+\n",
            "|2025-07-13 01:04:27.546|21   |RECEIVED  |1b692875-885d-432d-9e09-9c7d6f6bdca3|EMAIL  |2015      |1038   |\n",
            "|2025-07-13 01:04:35.546|29   |RECEIVED  |4a7c1c3c-7a65-4a30-9261-c365bd2807a4|EMAIL  |2006      |1050   |\n",
            "|2025-07-13 01:04:25.546|19   |RECEIVED  |e3347406-92a7-42ba-b100-8cfc4cccc2ce|EMAIL  |2004      |1015   |\n",
            "|2025-07-13 01:04:17.546|11   |RECEIVED  |f6a9c10c-b23e-468a-b552-f3a017096d27|OTHER  |2006      |1047   |\n",
            "|2025-07-13 01:04:59.546|53   |CREATED   |22f79baa-f315-40c6-85c3-35efa53521d6|EMAIL  |2012      |1026   |\n",
            "|2025-07-13 01:04:28.546|22   |CREATED   |697e2c39-0c25-42e7-950b-97d11dd2edbb|EMAIL  |2011      |1007   |\n",
            "|2025-07-13 01:04:51.546|45   |RECEIVED  |088403c0-92f8-4f5d-b15c-340388bfa233|PUSH   |2006      |1007   |\n",
            "|2025-07-13 01:04:58.546|52   |CLICKED   |0ae1a7fc-0b7c-44c6-b19c-028eac61dfeb|EMAIL  |2011      |1026   |\n",
            "|2025-07-13 01:04:20.546|14   |RECEIVED  |9a0b0d1a-914e-43ea-b3fa-7aed72a7545a|CHAT   |2015      |1002   |\n",
            "|2025-07-13 01:04:23.546|17   |CLICKED   |7edb5e7c-84a2-43ff-bef2-240964aca7f8|OTHER  |2007      |1010   |\n",
            "+-----------------------+-----+----------+------------------------------------+-------+----------+-------+\n",
            "only showing top 10 rows\n",
            "\n",
            "Step 1 completed successfully!\n",
            "Producer cleanup completed\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[timestamp: timestamp, value: bigint, event_type: string, message_id: string, channel: string, country_id: int, user_id: int]"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RraxHCycMdEZ"
      },
      "source": [
        "# Additional datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cfsus3dxMcQI"
      },
      "outputs": [],
      "source": [
        "countries = [\n",
        "    {\"country_id\": 2000, \"country\": \"Brazil\"},\n",
        "    {\"country_id\": 2001, \"country\": \"Portugal\"},\n",
        "    {\"country_id\": 2002, \"country\": \"Spain\"},\n",
        "    {\"country_id\": 2003, \"country\": \"Germany\"},\n",
        "    {\"country_id\": 2004, \"country\": \"France\"},\n",
        "    {\"country_id\": 2005, \"country\": \"Italy\"},\n",
        "    {\"country_id\": 2006, \"country\": \"United Kingdom\"},\n",
        "    {\"country_id\": 2007, \"country\": \"United States\"},\n",
        "    {\"country_id\": 2008, \"country\": \"Canada\"},\n",
        "    {\"country_id\": 2009, \"country\": \"Australia\"},\n",
        "    {\"country_id\": 2010, \"country\": \"Japan\"},\n",
        "    {\"country_id\": 2011, \"country\": \"China\"},\n",
        "    {\"country_id\": 2012, \"country\": \"India\"},\n",
        "    {\"country_id\": 2013, \"country\": \"South Korea\"},\n",
        "    {\"country_id\": 2014, \"country\": \"Russia\"},\n",
        "    {\"country_id\": 2015, \"country\": \"Argentina\"}\n",
        "]\n",
        "\n",
        "countries = spark.createDataFrame(countries)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pg2nx03_Sn62"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# === CHALLENGE 1 - STEP 2 ===\n"
      ],
      "metadata": {
        "id": "cIdz0NG51IKh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swvPj9hVpzNf"
      },
      "source": [
        "# Streaming Messages x Messages Corrupted"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import shutil\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "# =====================================\n",
        "# Countries Dataset Setup\n",
        "# =====================================\n",
        "def create_countries_dataset():\n",
        "    \"\"\"\n",
        "    Create static dataset mapping country_id to country name.\n",
        "    \"\"\"\n",
        "    countries = [\n",
        "        {\"country_id\": 2000, \"country\": \"Brazil\"},\n",
        "        {\"country_id\": 2001, \"country\": \"Portugal\"},\n",
        "        {\"country_id\": 2002, \"country\": \"Spain\"},\n",
        "        {\"country_id\": 2003, \"country\": \"Germany\"},\n",
        "        {\"country_id\": 2004, \"country\": \"France\"},\n",
        "        {\"country_id\": 2005, \"country\": \"Italy\"},\n",
        "        {\"country_id\": 2006, \"country\": \"United Kingdom\"},\n",
        "        {\"country_id\": 2007, \"country\": \"United States\"},\n",
        "        {\"country_id\": 2008, \"country\": \"Canada\"},\n",
        "        {\"country_id\": 2009, \"country\": \"Australia\"},\n",
        "        {\"country_id\": 2010, \"country\": \"Japan\"},\n",
        "        {\"country_id\": 2011, \"country\": \"China\"},\n",
        "        {\"country_id\": 2012, \"country\": \"India\"},\n",
        "        {\"country_id\": 2013, \"country\": \"South Korea\"},\n",
        "        {\"country_id\": 2014, \"country\": \"Russia\"},\n",
        "        {\"country_id\": 2015, \"country\": \"Argentina\"}\n",
        "    ]\n",
        "\n",
        "    countries_df = spark.createDataFrame(countries)\n",
        "    return countries_df\n",
        "\n",
        "# =====================================\n",
        "# Define schema for bronze layer data\n",
        "# =====================================\n",
        "def define_bronze_schema():\n",
        "    \"\"\"\n",
        "    Define the expected schema for messages in the bronze layer.\n",
        "    Ensures consistent parsing of timestamp, IDs, and event attributes.\n",
        "    \"\"\"\n",
        "    return StructType([\n",
        "        StructField(\"timestamp\", TimestampType(), True),\n",
        "        StructField(\"value\", LongType(), True),\n",
        "        StructField(\"event_type\", StringType(), True),\n",
        "        StructField(\"message_id\", StringType(), True),\n",
        "        StructField(\"channel\", StringType(), True),\n",
        "        StructField(\"country_id\", IntegerType(), True),\n",
        "        StructField(\"user_id\", IntegerType(), True)\n",
        "    ])\n",
        "\n",
        "# ---------------------------------------------\n",
        "# Clean up and prepare silver layer directories\n",
        "# ---------------------------------------------\n",
        "def setup_silver_directories():\n",
        "    \"\"\"\n",
        "    Clean up and recreate silver layer directories.\n",
        "    Ensures clean separation of valid and corrupted messages.\n",
        "    \"\"\"\n",
        "    silver_paths = [\n",
        "        \"content/lake/silver/messages\",\n",
        "        \"content/lake/silver/messages_corrupted\",\n",
        "        \"content/lake/silver/etl_checkpoint\"\n",
        "    ]\n",
        "\n",
        "    for path in silver_paths:\n",
        "        if os.path.exists(path):\n",
        "            shutil.rmtree(path)\n",
        "\n",
        "    os.makedirs(\"content/lake/silver/messages/data\", exist_ok=True)\n",
        "    os.makedirs(\"content/lake/silver/messages_corrupted/data\", exist_ok=True)\n",
        "    os.makedirs(\"content/lake/silver/etl_checkpoint/checkpoint\", exist_ok=True)\n",
        "\n",
        "# =====================================\n",
        "# Foreachbatch processing\n",
        "# =====================================\n",
        "def process_batch_to_both_sinks(batch_df, batch_id):\n",
        "    \"\"\"\n",
        "    Split incoming micro-batch into corrupted and clean messages.\n",
        "    Enrich with country name and write each set to its respective silver path.\n",
        "    \"\"\"\n",
        "    if batch_df.isEmpty():\n",
        "        print(f\"üì≠ Batch {batch_id}: No data\")\n",
        "        return\n",
        "\n",
        "    countries_df = create_countries_dataset()\n",
        "\n",
        "    enriched_batch = batch_df.join(\n",
        "        broadcast(countries_df), \"country_id\", \"left\"\n",
        "    ).withColumn(\"country_name\", col(\"country\")).drop(\"country\")\n",
        "\n",
        "    enriched_batch = enriched_batch.withColumn(\n",
        "        \"date\", date_format(col(\"timestamp\"), \"yyyy-MM-dd\")\n",
        "    )\n",
        "\n",
        "    corrupted = enriched_batch.filter(col(\"event_type\").isNull() | (col(\"event_type\") == \"\") | (col(\"event_type\") == \"NONE\"))\n",
        "    clean = enriched_batch.filter(~(col(\"event_type\").isNull() | (col(\"event_type\") == \"\") | (col(\"event_type\") == \"NONE\")))\n",
        "\n",
        "    if not corrupted.isEmpty():\n",
        "        corrupted.write.mode(\"append\").partitionBy(\"date\").parquet(\"content/lake/silver/messages_corrupted/data\")\n",
        "    if not clean.isEmpty():\n",
        "        clean.write.mode(\"append\").partitionBy(\"date\").parquet(\"content/lake/silver/messages/data\")\n",
        "\n",
        "# ----------------------------------------\n",
        "# Streaming query setup and execution\n",
        "# ----------------------------------------\n",
        "def start_single_stream_with_foreach():\n",
        "    \"\"\"\n",
        "    Start structured streaming query using foreachBatch logic.\n",
        "    Reads from bronze and routes to silver based on corruption logic.\n",
        "    \"\"\"\n",
        "    setup_silver_directories()\n",
        "\n",
        "    bronze_schema = define_bronze_schema()\n",
        "\n",
        "    bronze_stream = spark.readStream \\\n",
        "        .format(\"parquet\") \\\n",
        "        .schema(bronze_schema) \\\n",
        "        .load(\"content/lake/bronze/messages/data\")\n",
        "\n",
        "    return bronze_stream.writeStream \\\n",
        "        .foreachBatch(process_batch_to_both_sinks) \\\n",
        "        .option(\"checkpointLocation\", \"content/lake/silver/etl_checkpoint/checkpoint\") \\\n",
        "        .trigger(processingTime=\"5 seconds\") \\\n",
        "        .start()\n",
        "\n",
        "# ----------------------------------------\n",
        "# Resource cleanup\n",
        "# ----------------------------------------\n",
        "def cleanup_etl_resources():\n",
        "    \"\"\"\n",
        "    Stops all active Spark streaming queries to free resources.\n",
        "    Prints a warning if unable to stop any stream.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        active_streams = spark.streams.active\n",
        "        if active_streams:\n",
        "            for stream in active_streams:\n",
        "                try:\n",
        "                    stream.stop()\n",
        "                except Exception as e:\n",
        "                    print(f\"‚ö†Ô∏è  Warning stopping stream {stream.id}: {e}\")\n",
        "        print(\"ETL cleanup completed\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è  Warning during ETL cleanup: {e}\")\n",
        "\n",
        "# =====================================\n",
        "# MAIN EXECUTION\n",
        "# =====================================\n",
        "def run_etl_step2(run_time=20):\n",
        "    \"\"\"\n",
        "    Executes Step 2 of Challenge 1: Main execution for Bronze to Silver ETL pipeline using foreachBatch.\n",
        "    Only runs the streaming process and stops it after run_time seconds.\n",
        "    \"\"\"\n",
        "    print(\"Executing Challenge 1 - Step 2: Bronze to Silver Layer\")\n",
        "\n",
        "    try:\n",
        "        query = start_single_stream_with_foreach()\n",
        "        time.sleep(run_time)\n",
        "        query.stop()\n",
        "        print(\"‚úÖ Stream stopped successfully after ETL run\")\n",
        "\n",
        "        # Quick status\n",
        "        try:\n",
        "            clean_count = spark.read.parquet(\"content/lake/silver/messages/data\").count()\n",
        "            corrupted_count = spark.read.parquet(\"content/lake/silver/messages_corrupted/data\").count()\n",
        "            print(f\"Quick results: {clean_count} clean, {corrupted_count} corrupted\")\n",
        "        except:\n",
        "            print(\"Data still processing or not available yet\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå ETL failed: {e}\")\n",
        "        raise e\n",
        "    finally:\n",
        "        cleanup_etl_resources()"
      ],
      "metadata": {
        "id": "6Ffthm1PQBGd"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_etl_step2()"
      ],
      "metadata": {
        "id": "OGbeiGPBSGML",
        "outputId": "93edb24d-209f-44e2-bc18-4af5abf6964a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Executing Challenge 1 - Step 2: Bronze to Silver Layer\n",
            "‚úÖ Stream stopped successfully after ETL run\n",
            "Quick results: 43 clean, 16 corrupted\n",
            "ETL cleanup completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLK9jpjCu3xE"
      },
      "source": [
        "## Checking data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "nk8seEvbmvcU"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# ----------------------------\n",
        "# Count and compare all layers\n",
        "# ----------------------------\n",
        "def validate_counts():\n",
        "    \"\"\"\n",
        "    Compare bronze layer record count with total of silver clean + corrupted.\n",
        "    Return summary with counts and validation result.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        bronze_count = spark.read.parquet(\"content/lake/bronze/messages/data\").count()\n",
        "    except:\n",
        "        print(\"‚ùå Error reading bronze layer\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        clean_count = spark.read.parquet(\"content/lake/silver/messages/data\").count()\n",
        "    except:\n",
        "        clean_count = 0\n",
        "\n",
        "    try:\n",
        "        corrupted_count = spark.read.parquet(\"content/lake/silver/messages_corrupted/data\").count()\n",
        "    except:\n",
        "        corrupted_count = 0\n",
        "\n",
        "    silver_total = clean_count + corrupted_count\n",
        "    result = {\n",
        "        \"bronze\": bronze_count,\n",
        "        \"clean\": clean_count,\n",
        "        \"corrupted\": corrupted_count,\n",
        "        \"silver_total\": silver_total,\n",
        "        \"match\": bronze_count == silver_total\n",
        "    }\n",
        "\n",
        "    print(\"\\nüìä COUNT SUMMARY\")\n",
        "    print(f\"Bronze:           {bronze_count}\")\n",
        "    print(f\"Silver Clean:     {clean_count}\")\n",
        "    print(f\"Silver Corrupted: {corrupted_count}\")\n",
        "    print(f\"Silver Total:     {silver_total}\")\n",
        "    print(f\"Match:            {'‚úÖ YES' if result['match'] else '‚ùå NO'}\")\n",
        "\n",
        "    return result\n",
        "\n",
        "# ----------------------------\n",
        "# Show 5 sample records\n",
        "# ----------------------------\n",
        "def show_samples():\n",
        "    \"\"\"Display 5 records from each layer for quick inspection.\"\"\"\n",
        "    try:\n",
        "        print(\"\\nüü¶ Bronze Sample:\")\n",
        "        spark.read.parquet(\"content/lake/bronze/messages/data\").show(5, truncate=False)\n",
        "    except:\n",
        "        print(\"No bronze data found\")\n",
        "\n",
        "    try:\n",
        "        print(\"\\nüü¢ Silver Clean Sample:\")\n",
        "        spark.read.parquet(\"content/lake/silver/messages/data\").show(5, truncate=False)\n",
        "    except:\n",
        "        print(\"No clean data found\")\n",
        "\n",
        "    try:\n",
        "        print(\"\\nüî¥ Silver Corrupted Sample:\")\n",
        "        spark.read.parquet(\"content/lake/silver/messages_corrupted/data\").show(5, truncate=False)\n",
        "    except:\n",
        "        print(\"No corrupted data found\")\n",
        "\n",
        "# ----------------------------\n",
        "# Validate Results\n",
        "# ----------------------------\n",
        "def validate_results():\n",
        "    \"\"\"Run validation.\"\"\"\n",
        "    result = validate_counts()\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "validate_results()"
      ],
      "metadata": {
        "id": "VpSlHqF_B1dZ",
        "outputId": "04a76476-868f-40c8-b4d8-615418a6c127",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä COUNT SUMMARY\n",
            "Bronze:           59\n",
            "Silver Clean:     43\n",
            "Silver Corrupted: 16\n",
            "Silver Total:     59\n",
            "Match:            ‚úÖ YES\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'bronze': 59, 'clean': 43, 'corrupted': 16, 'silver_total': 59, 'match': True}"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfxIlBISSvRP"
      },
      "source": [
        "# Challenge 2\n",
        "\n",
        "- Run business report\n",
        "- But first, there is a bug in the system which is causing some duplicated messages, we need to exclude these lines from the report\n",
        "\n",
        "- removing duplicates logic:\n",
        "  - Identify possible duplicates on message_id, event_type and channel\n",
        "  - in case of duplicates, consider only the first message (occurrence by timestamp)\n",
        "  - Ex:\n",
        "    In table below, the correct message to consider is the second line\n",
        "\n",
        "```\n",
        "    message_id | channel | event_type | timestamp\n",
        "    123        | CHAT    | CREATED    | 10:10:01\n",
        "    123        | CHAT    | CREATED    | 07:56:45 (first occurrence)\n",
        "    123        | CHAT    | CREATED    | 08:13:33\n",
        "```\n",
        "\n",
        "- After cleaning the data we're able to create the busines report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R3J9XyOHhqvU"
      },
      "outputs": [],
      "source": [
        "# dedup data\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.window import Window\n",
        "df = spark.read.format(\"parquet\").load(\"content/lake/silver/messages\")\n",
        "dedup = df.withColumn(\"row_number\", F.row_number().over(Window.partitionBy(\"message_id\", \"event_type\", \"channel\").orderBy(\"timestamp\"))).filter(\"row_number = 1\").drop(\"row_number\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RF9L9i25lk74"
      },
      "source": [
        "### Report 1\n",
        "  - Aggregate data by date, event_type and channel\n",
        "  - Count number of messages\n",
        "  - pivot event_type from rows into columns\n",
        "  - schema expected:\n",
        "  \n",
        "```\n",
        "|      date|channel|CLICKED|CREATED|OPEN|RECEIVED|SENT|\n",
        "+----------+-------+-------+-------+----+--------+----+\n",
        "|2024-12-03|    SMS|      4|      4|   1|       1|   5|\n",
        "|2024-12-03|   CHAT|      3|      7|   5|       8|   4|\n",
        "|2024-12-03|   PUSH|   NULL|      3|   4|       3|   4|\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UPHSMSXnTKgu"
      },
      "outputs": [],
      "source": [
        "# report 1\n",
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxwOawo2lwQH"
      },
      "source": [
        "## Report 2\n",
        "\n",
        "- Identify the most active users by channel (sorted by number of iterations)\n",
        "- schema expected:\n",
        "\n",
        "```\n",
        "+-------+----------+----+-----+-----+----+---+\n",
        "|user_id|iterations|CHAT|EMAIL|OTHER|PUSH|SMS|\n",
        "+-------+----------+----+-----+-----+----+---+\n",
        "|   1022|         5|   2|    0|    1|   0|  2|\n",
        "|   1004|         4|   1|    1|    1|   1|  0|\n",
        "|   1013|         4|   0|    0|    2|   1|  1|\n",
        "|   1020|         4|   2|    0|    1|   1|  0|\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rsS7bkAJmWsW"
      },
      "outputs": [],
      "source": [
        "# report 2\n",
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9_kzDbDwDOS"
      },
      "source": [
        "# Challenge 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ef0RjFTxwE5y"
      },
      "outputs": [],
      "source": [
        "# Theoretical question:\n",
        "\n",
        "# A new usecase requires the message data to be aggregate in near real time\n",
        "# They want to build a dashboard embedded in the platform website to analyze message data in low latency (few minutes)\n",
        "# This application will access directly the data aggregated by streaming process\n",
        "\n",
        "# Q1:\n",
        "- What would be your suggestion to achieve that using Spark Structure Streaming?\n",
        "Or would you choose a different data processing tool?\n",
        "\n",
        "# Q2:\n",
        "- Which storage would you use and why? (database?, data lake?, kafka?)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# agrega√ß√£o em tempo real e no fim ter um dashboard em tempo real com os resultados (baixa lat√™ncia)\n",
        "# o engine de consumo dos dados agregados tem que ser r√°pido para aceder e mostrar esses mesmos dados\n",
        "# qual o storage? bd? datalake? kafka?\n",
        "# usariamos o spark? ou outra forma de processar dados?\n",
        "# testar l√≥gica de arquitetura"
      ],
      "metadata": {
        "id": "vFmx2kkx0oIu"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}